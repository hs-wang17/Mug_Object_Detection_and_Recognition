{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e19c62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm, trange"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fdad05",
   "metadata": {},
   "source": [
    "# 读取数据\n",
    "数据来源：scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0012563",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02506d395587469297302e33164848eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e032ea87279d46ada66d6d7c43b3e156",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_list = []\n",
    "y_list = []\n",
    "for pic in tqdm(os.listdir('scene')):\n",
    "    pic_path = './scene/' + pic\n",
    "    # 读取RGB三通道图像(640, 640, 3)\n",
    "    pic_data = cv2.imread(pic_path, cv2.IMREAD_COLOR)\n",
    "    pic_data = cv2.resize(pic_data, (640, 640))\n",
    "    x_list.append(pic_data)\n",
    "    y_list.append(int(pic[6:8]))\n",
    "x_list = np.array(x_list)  # scene: (2500, 640, 1137, 3), object: (15000, 640, 640, 3)\n",
    "y_list_int = np.array(y_list)\n",
    "\n",
    "scene_label = pd.read_excel('scene_label.xlsx')\n",
    "y_list = np.zeros((y_list_int.shape[0], 20))\n",
    "for i in trange(y_list_int.shape[0]):\n",
    "    y_list[i, scene_label[scene_label.id==y_list_int[i]].iloc[:, 1:].dropna(axis=1).astype(int).to_numpy()[0].tolist()] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebdcad7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# 假设标签数据保存在label_list中，其中每个标签是一个整数\n",
    "X = x_list\n",
    "y = y_list\n",
    "\n",
    "# 分层抽样，其中train_size和test_size分别表示训练集和测试集的比例\n",
    "# n_splits表示抽取的次数，random_state表示随机数种子\n",
    "split = StratifiedShuffleSplit(n_splits=1, train_size=0.8, test_size=0.2, random_state=42)\n",
    "train_index, test_index = next(split.split(X, y))\n",
    "\n",
    "# 得到训练集和测试集\n",
    "X_train, X_test = [X[i] for i in train_index], [X[i] for i in test_index]\n",
    "y_train, y_test = [y[i] for i in train_index], [y[i] for i in test_index]\n",
    "\n",
    "# 将训练集进一步划分为训练集和验证集，其中test_size表示验证集的比例\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_index, valid_index = next(split.split(X_train, y_train))\n",
    "\n",
    "# 得到训练集、验证集和测试集的索引\n",
    "train_index = [train_index[i] for i in range(len(train_index))]\n",
    "valid_index = [valid_index[i] for i in range(len(valid_index))]\n",
    "test_index = [test_index[i] for i in range(len(test_index))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff96fb06",
   "metadata": {},
   "source": [
    "# 分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf3eb630",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.datasets import ImageFolder\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "# 定义超参数\n",
    "batch_size = 8\n",
    "learning_rate = 0.0001\n",
    "num_epochs = 100\n",
    "\n",
    "# 设置 GPU\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, labels, transform=None):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = self.data[index]\n",
    "        label = self.labels[index]\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "# 加载数据集\n",
    "train_data = x_list[train_index]\n",
    "train_labels = y_list[train_index]\n",
    "valid_data = x_list[valid_index]\n",
    "valid_labels = y_list[valid_index]\n",
    "test_data = x_list[test_index]\n",
    "test_labels = y_list[test_index]\n",
    "\n",
    "# # 定义数据增强和标准化\n",
    "# # 在scene数据集中只做了标准化\n",
    "transform = transforms.Compose([\n",
    "#     transforms.RandomAffine(5),\n",
    "#     transforms.ColorJitter(hue=.05, saturation=.05),\n",
    "#     transforms.RandomCrop((88, 88)),\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "#     transforms.RandomVerticalFlip(),\n",
    "    transforms.ToTensor(), # 转换为张量\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # 标准化张量\n",
    "])\n",
    "\n",
    "# 加载数据集\n",
    "train_dataset = MyDataset(train_data, train_labels, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_dataset = MyDataset(valid_data, valid_labels, transform=transform)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataset = MyDataset(test_data, test_labels, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e57078a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, num_classes=19):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(in_features=128 * 80 * 80, out_features=512)\n",
    "        self.fc2 = nn.Linear(in_features=512, out_features=num_classes)\n",
    "#         self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, 128 * 80 * 80)\n",
    "        x = F.relu(self.fc1(x))\n",
    "#         x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = MyModel(num_classes=20)\n",
    "# model.to(device)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f6eca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()\n",
    "writer.add_graph(model, input_to_model=torch.rand(batch_size, 3, 640, 640,).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d293ca9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6e7f421a8c64f7588a171543adfae5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c9be0a832d54d3b8ad21d6dee2609cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 10.1670\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f96d6d2c5c2b4596be0b5e9603310462",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of the model on the valid images: 8.588871\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eaef1c2162a4c6283c2f63f8e179763",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/100], Loss: 7.5190\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "076bfb393e6a464f97162904b7057b47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of the model on the valid images: 7.337446\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "597edd3cac724c7a9d4d6f105609ee68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/100], Loss: 6.2755\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "307aa974a3e943babee8caab4a100a41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of the model on the valid images: 4.638095\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f9bfba9754b4113b8e524ca0c245ae7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/100], Loss: 5.7667\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eebafcb4cd004dd793d5d1c09f5a0c1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of the model on the valid images: 5.780078\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d145b80b85554e69b03aed878731b96a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/100], Loss: 5.4898\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "398efbdaffc74a13875b9d150d15fe7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of the model on the valid images: 5.996552\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e83ca69c8a241f396531bca66e7f72d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/100], Loss: 5.3365\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2489662992c74a41bded0bb12de8d435",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of the model on the valid images: 5.007970\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0755c1a1e2b747b98d379a4cd0cecf37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/100], Loss: 5.2332\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d6e3231b05843598e0c50bb21eb6b96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of the model on the valid images: 4.813715\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cb74d5b4d034959a1b51f907415805b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/100], Loss: 5.1673\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "559ddb9956f24930bd3fad0380bf4845",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of the model on the valid images: 3.832993\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dee83fc6204043d18f5bf77b6a0ba174",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/100], Loss: 5.1234\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44654816d52a43739fa5fd9bb401ed3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of the model on the valid images: 4.158244\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a7ae469436544edacccdb855f51aaf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 5.1087\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd97cfe475a94afa932101e480c4c209",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of the model on the valid images: 4.804686\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7097c7c88c444e88b10ac42b6b55bc22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 训练模型\n",
    "for epoch in trange(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    model.train()\n",
    "    for inputs, labels in tqdm(train_loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    print('Epoch [%d/%d], Loss: %.4f' % (epoch + 1, num_epochs, epoch_loss))\n",
    "    torch.save(model.state_dict(), \"./classifier/model-CNN-leakyrelu-scene/epoch-%d.pt\" % epoch)\n",
    "    writer.add_scalar('Loss/train', epoch_loss, epoch)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in tqdm(valid_loader):\n",
    "            inputs, labels = inputs.to(device), labels\n",
    "            outputs = model(inputs)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        epoch_loss = running_loss / len(valid_dataset)\n",
    "        print('Loss of the model on the valid images: %f' % loss)\n",
    "    writer.add_scalar('Loss/valid', epoch_loss, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffc828f",
   "metadata": {},
   "source": [
    "# 评价"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c136e50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a242f6543544e4ba00c64a65f06381d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = []\n",
    "y_true = []\n",
    "# 测试模型（这里是针对单个杯子设计的准确率，多个杯子需修改代码）\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in tqdm(test_loader):\n",
    "        outputs = model(inputs)\n",
    "        predicted = np.int64(np.array(outputs) > 0)\n",
    "        y_pred.append(predicted)\n",
    "        y_true.append(np.int64(np.array(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ea30359",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.row_stack(y_pred)\n",
    "y_true = np.row_stack(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6af9cf0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multilabel Confusion Matrix:\n",
      "[[[459   1]\n",
      "  [  2  38]]\n",
      "\n",
      " [[356   4]\n",
      "  [  4 136]]\n",
      "\n",
      " [[371   9]\n",
      "  [  9 111]]\n",
      "\n",
      " [[393   7]\n",
      "  [  9  91]]\n",
      "\n",
      " [[394   6]\n",
      "  [  7  93]]\n",
      "\n",
      " [[437   3]\n",
      "  [  6  54]]\n",
      "\n",
      " [[373   7]\n",
      "  [  8 112]]\n",
      "\n",
      " [[286  14]\n",
      "  [  2 198]]\n",
      "\n",
      " [[378   2]\n",
      "  [  1 119]]\n",
      "\n",
      " [[391   9]\n",
      "  [  3  97]]\n",
      "\n",
      " [[368  12]\n",
      "  [  7 113]]\n",
      "\n",
      " [[376   4]\n",
      "  [  2 118]]\n",
      "\n",
      " [[480   0]\n",
      "  [  5  15]]\n",
      "\n",
      " [[435   5]\n",
      "  [  0  60]]\n",
      "\n",
      " [[408  12]\n",
      "  [  1  79]]\n",
      "\n",
      " [[456   4]\n",
      "  [  0  40]]\n",
      "\n",
      " [[397   3]\n",
      "  [  1  99]]\n",
      "\n",
      " [[431   9]\n",
      "  [  1  59]]\n",
      "\n",
      " [[452   8]\n",
      "  [  0  40]]\n",
      "\n",
      " [[409  11]\n",
      "  [  0  80]]]\n",
      "Precision: 0.9309245483528161\n",
      "Recall: 0.9626373626373627\n",
      "F1 Score: 0.946515397082658\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "# 计算混淆矩阵\n",
    "mcm = multilabel_confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# 计算精确度、召回率和F1分数\n",
    "precision = precision_score(y_true, y_pred, average='micro')\n",
    "recall = recall_score(y_true, y_pred, average='micro')\n",
    "f1 = f1_score(y_true, y_pred, average='micro')\n",
    "\n",
    "# 打印结果\n",
    "print(\"Multilabel Confusion Matrix:\")\n",
    "print(mcm)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d76b58c",
   "metadata": {},
   "source": [
    "# 绘制中间特征层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1bc72b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = './classifier/model-CNN-scene/epoch-99.pt'\n",
    "trained_state_dict = torch.load(path)\n",
    "model.load_state_dict(trained_state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "9bfc7a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hooker working <__main__.Hook object at 0x7f3324b916d0>\n",
      "hooker working <__main__.Hook object at 0x7f3325775dc0>\n",
      "hooker working <__main__.Hook object at 0x7f33257cfbe0>\n",
      "hooker working <__main__.Hook object at 0x7f3324adf460>\n",
      "hooker working <__main__.Hook object at 0x7f33248b9550>\n",
      "hooker working <__main__.Hook object at 0x7f3324710be0>\n",
      "hooker working <__main__.Hook object at 0x7f3323aa3eb0>\n"
     ]
    }
   ],
   "source": [
    "class Hook(object):\n",
    "    def __init__(self):\n",
    "        self.module_name = []\n",
    "        self.features_in_hook = []\n",
    "        self.features_out_hook = []\n",
    "\n",
    "    def __call__(self, module, fea_in, fea_out):\n",
    "        print(\"hooker working\", self)\n",
    "        self.module_name.append(module.__class__)\n",
    "        self.features_in_hook.append(fea_in)\n",
    "        self.features_out_hook.append(fea_out)\n",
    "        return None\n",
    "    \n",
    "hh = Hook()\n",
    "model.eval()\n",
    "model.conv3.register_forward_hook(hh)\n",
    "\n",
    "inputs = test_dataset[0][0]\n",
    "outputs = model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "9c762dde",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<class 'torch.nn.modules.linear.Linear'>]\n",
      "torch.Size([1, 819200])\n",
      "torch.Size([1, 512])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Invalid shape (512,) for image data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-129-17aca5b936bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mftidx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'off'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fc1.pdf'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox_inches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tight'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, filternorm, filterrad, resample, url, data, **kwargs)\u001b[0m\n\u001b[1;32m   2722\u001b[0m         \u001b[0mfilternorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilterrad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2723\u001b[0m         data=None, **kwargs):\n\u001b[0;32m-> 2724\u001b[0;31m     __ret = gca().imshow(\n\u001b[0m\u001b[1;32m   2725\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maspect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maspect\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2726\u001b[0m         \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1446\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1447\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5521\u001b[0m                               resample=resample, **kwargs)\n\u001b[1;32m   5522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5523\u001b[0;31m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5524\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5525\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mset_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    709\u001b[0m         if not (self._A.ndim == 2\n\u001b[1;32m    710\u001b[0m                 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):\n\u001b[0;32m--> 711\u001b[0;31m             raise TypeError(\"Invalid shape {} for image data\"\n\u001b[0m\u001b[1;32m    712\u001b[0m                             .format(self._A.shape))\n\u001b[1;32m    713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid shape (512,) for image data"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEkAAABJCAYAAABxcwvcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAAsklEQVR4nO3QwQkAIRDAwPP673ntQPITYaaCkDUzH2f/7YAXmBSYFJgUmBSYFJgUmBSYFJgUmBSYFJgUmBSYFJgUmBSYFJgUmBSYFJgUmBSYFJgUmBSYFJgUmBSYFJgUmBSYFJgUmBSYFJgUmBSYFJgUmBSYFJgUmBSYFJgUmBSYFJgUmBSYFJgUmBSYFJgUmBSYFJgUmBSYFJgUmBSYFJgUmBSYFJgUmBSYFJgUmBSYFGzdIAOPZLqw2AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(hh.module_name)\n",
    "print((hh.features_in_hook[0][0].shape))\n",
    "print((hh.features_out_hook[0].shape))\n",
    "\n",
    "out1 = hh.features_out_hook[0]\n",
    "total_ft  = out1.shape[0]\n",
    "first_item = out1.cpu().clone()    \n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "for ftidx in range(min(total_ft, 32)):\n",
    "    ft = first_item[ftidx]\n",
    "    plt.subplot(4, 8, ftidx + 1) \n",
    "    plt.axis('off')\n",
    "    plt.imshow(ft.detach(), cmap='gray')\n",
    "plt.savefig('conv3.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1aa5adc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyModel(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=819200, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=20, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "5e870086",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 32 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "for ftidx in range(32):\n",
    "    plt.subplot(4, 8, ftidx + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(trained_state_dict['conv1.weight'].cpu()[ftidx].numpy()[0])\n",
    "plt.savefig('conv1_kernel.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "45ceb799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 32 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "for ftidx in range(32):\n",
    "    plt.subplot(4, 8, ftidx + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(trained_state_dict['conv2.weight'].cpu()[ftidx].numpy()[0])\n",
    "plt.savefig('conv2_kernel.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "808d6463",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 32 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "for ftidx in range(32):\n",
    "    plt.subplot(4, 8, ftidx + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(trained_state_dict['conv3.weight'].cpu()[ftidx].numpy()[0])\n",
    "plt.savefig('conv3_kernel.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "a62891ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d610f488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an explainer with model and image masker\n",
    "explainer = shap.Explainer(f, masker, output_names=class_names)\n",
    "\n",
    "# here we explain two images using 500 evaluations of the underlying model to estimate the SHAP values\n",
    "shap_values = explainer(X[1:3], max_evals=100, batch_size=50, outputs=shap.Explanation.argsort.flip[:4])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
